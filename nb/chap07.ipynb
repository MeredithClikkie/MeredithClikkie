{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MeredithClikkie/MeredithClikkie/blob/main/nb/chap07.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "418e407e",
      "metadata": {
        "id": "418e407e"
      },
      "source": [
        "The third edition of *Think Stats* is available now from [Bookshop.org](https://bookshop.org/a/98697/9781098190255) and [Amazon](https://amzn.to/42lmxwu) (those are affiliate links). If you are enjoying the free, online version, consider [buying me a coffee](https://buymeacoffee.com/allendowney)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79cfee17",
      "metadata": {
        "id": "79cfee17"
      },
      "source": [
        "# Relationships between variables\n",
        "\n",
        "So far we have only looked at one variable at a time.\n",
        "In this chapter we start looking at relationships between variables.\n",
        "Two variables are related if knowing one gives you information about the other.\n",
        "For example, height and weight are related -- people who are taller tend to be heavier.\n",
        "Of course, it is not a perfect relationship: there are short heavy people and tall light ones.\n",
        "But if you are trying to guess someone's weight, you will be more accurate if you know their height than if you don't.\n",
        "\n",
        "This chapter presents several ways to visualize relationships between variables, and one way to quantify the strength of a relationship, correlation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f4cd1fe",
      "metadata": {
        "tags": [],
        "id": "8f4cd1fe"
      },
      "source": [
        "[Click here to run this notebook on Colab](https://colab.research.google.com/github/AllenDowney/ThinkStats/blob/v3/nb/chap07.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "542c6943",
      "metadata": {
        "tags": [],
        "id": "542c6943",
        "outputId": "883ff031-85bc-4774-ca83-7405e64076b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded thinkstats.py\n"
          ]
        }
      ],
      "source": [
        "from os.path import basename, exists\n",
        "\n",
        "\n",
        "def download(url):\n",
        "    filename = basename(url)\n",
        "    if not exists(filename):\n",
        "        from urllib.request import urlretrieve\n",
        "\n",
        "        local, _ = urlretrieve(url, filename)\n",
        "        print(\"Downloaded \" + local)\n",
        "\n",
        "\n",
        "download(\"https://github.com/AllenDowney/ThinkStats/raw/v3/nb/thinkstats.py\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f41c5c5",
      "metadata": {
        "tags": [],
        "id": "7f41c5c5",
        "outputId": "18eccc17-4804-40e9-9c8a-1c764c1f9370",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting empiricaldist\n",
            "  Downloading empiricaldist-0.9.0.tar.gz (14 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from empiricaldist) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from empiricaldist) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from empiricaldist) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from empiricaldist) (1.16.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->empiricaldist) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->empiricaldist) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->empiricaldist) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->empiricaldist) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->empiricaldist) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->empiricaldist) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->empiricaldist) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->empiricaldist) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->empiricaldist) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->empiricaldist) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->empiricaldist) (1.17.0)\n",
            "Building wheels for collected packages: empiricaldist\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import empiricaldist\n",
        "except ImportError:\n",
        "    %pip install empiricaldist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c826e189",
      "metadata": {
        "tags": [],
        "id": "c826e189"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from thinkstats import decorate"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4945e44",
      "metadata": {
        "id": "d4945e44"
      },
      "source": [
        "## Scatter Plots\n",
        "\n",
        "If you meet someone who is unusually good at math, do you expect their verbal skills to be better or worse than average?\n",
        "On one hand, you might imagine that people specialize in one area or the other, so someone who excels at one might be less good at the other.\n",
        "On the other hand, you might expect someone who is generally smart to be above average in both areas.\n",
        "Let's find out which it is.\n",
        "\n",
        "We'll use data from the National Longitudinal Survey of Youth 1997 (NLSY97), which \"follows the lives of a sample of 8,984 American youth born between 1980-84\".\n",
        "The public data set includes the participants' scores on several standardized tests, including the tests most often used in college admissions, the SAT and ACT.\n",
        "Because test-takers get separate scores for the math and verbal sections, we can use this data to explore the relationship between mathematical and verbal ability.\n",
        "\n",
        "I used the NLS Investigator to create an excerpt that contains the variables I'll use for this analysis.\n",
        "With their permission, I can redistribute this excerpt.\n",
        "Instructions for downloading the data are in the notebook for this chapter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8e375cd",
      "metadata": {
        "tags": [],
        "id": "c8e375cd"
      },
      "outputs": [],
      "source": [
        "download(\"https://github.com/AllenDowney/ThinkStats/raw/v3/data/nlsy97-extract.csv.gz\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df42f753",
      "metadata": {
        "id": "df42f753"
      },
      "source": [
        "We can use `read_csv` to read the data and `replace` to replace the special codes for missing data with `np.nan`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5395bff6",
      "metadata": {
        "id": "5395bff6"
      },
      "outputs": [],
      "source": [
        "missing_codes = [-1, -2, -3, -4, -5]\n",
        "nlsy = pd.read_csv(\"nlsy97-extract.csv.gz\").replace(missing_codes, np.nan)\n",
        "nlsy.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80f09e49",
      "metadata": {
        "tags": [],
        "id": "80f09e49"
      },
      "outputs": [],
      "source": [
        "nlsy.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "164d0fd5",
      "metadata": {
        "id": "164d0fd5"
      },
      "source": [
        "The `DataFrame` contains one row for each of the 8984 participants in the survey and one column for each of the 34 variables I selected.\n",
        "The column names don't mean much by themselves, so let's replace the ones we'll use with more interpretable names."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fc65d7e",
      "metadata": {
        "id": "4fc65d7e"
      },
      "outputs": [],
      "source": [
        "nlsy[\"sat_verbal\"] = nlsy[\"R9793800\"]\n",
        "nlsy[\"sat_math\"] = nlsy[\"R9793900\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "123a6b38",
      "metadata": {
        "id": "123a6b38"
      },
      "source": [
        "Both columns contain a few values less than 200, which is not possible because 200 is the lowest score, so we'll replace them with `np.nan`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75f0085d",
      "metadata": {
        "id": "75f0085d"
      },
      "outputs": [],
      "source": [
        "columns = [\"sat_verbal\", \"sat_math\"]\n",
        "\n",
        "for column in columns:\n",
        "    invalid = nlsy[column] < 200\n",
        "    nlsy.loc[invalid, column] = np.nan"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41efbdaf",
      "metadata": {
        "id": "41efbdaf"
      },
      "source": [
        "Next we'll use `dropna` to select only rows where both scores are valid."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f3d8b49",
      "metadata": {
        "id": "6f3d8b49"
      },
      "outputs": [],
      "source": [
        "nlsy_valid = nlsy.dropna(subset=columns).copy()\n",
        "nlsy_valid.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16a23c87",
      "metadata": {
        "id": "16a23c87"
      },
      "source": [
        "SAT scores are standardized so the mean is 500 and the standard deviation is 100.\n",
        "In the NLSY sample, the means and standard deviations are close to these values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d31de9e",
      "metadata": {
        "id": "0d31de9e"
      },
      "outputs": [],
      "source": [
        "sat_verbal = nlsy_valid[\"sat_verbal\"]\n",
        "sat_verbal.mean(), sat_verbal.std()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a984b59",
      "metadata": {
        "id": "4a984b59"
      },
      "outputs": [],
      "source": [
        "sat_math = nlsy_valid[\"sat_math\"]\n",
        "sat_math.mean(), sat_math.std()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c079e47",
      "metadata": {
        "id": "3c079e47"
      },
      "source": [
        "Now, to see whether there is a relationship between these variables, let's look at a **scatter plot**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2e362ab",
      "metadata": {
        "id": "f2e362ab"
      },
      "outputs": [],
      "source": [
        "plt.scatter(sat_verbal, sat_math)\n",
        "\n",
        "decorate(xlabel=\"SAT Verbal\", ylabel=\"SAT Math\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39524848",
      "metadata": {
        "id": "39524848"
      },
      "source": [
        "Using the default options of the `scatter` function, we can see the general shape of the relationship.\n",
        "People who do well on one section of the test tend to do better on the other, too.\n",
        "\n",
        "However, this version of the figure is **overplotted**, which means there are a lot of overlapping points, which can create a misleading impression of the relationship.\n",
        "The center, where the density of points is highest, is not as dark as it should be -- by comparison, the extreme values are darker than they should be.\n",
        "Overplotting tends to give too much visual weight to outliers.\n",
        "\n",
        "We can improve the plot by reducing the size of the markers so they overlap less."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f658d7ae",
      "metadata": {
        "id": "f658d7ae"
      },
      "outputs": [],
      "source": [
        "plt.scatter(sat_verbal, sat_math, s=5)\n",
        "\n",
        "decorate(xlabel=\"SAT Verbal\", ylabel=\"SAT Math\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fac65084",
      "metadata": {
        "id": "fac65084"
      },
      "source": [
        "Now we can see that the markers are aligned in rows and columns, because scores are rounded off to the nearest multiple of 10.\n",
        "Some information is lost in the process.\n",
        "\n",
        "We can't get that information back, but we can minimize the effect on the scatter plot by **jittering** the data, which means adding random noise to reverse the effect of rounding off.\n",
        "The following function takes a sequence and jitters it by adding random values from a normal distribution with mean 0 and the given standard deviation.\n",
        "The result is a NumPy array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2de395a",
      "metadata": {
        "id": "d2de395a"
      },
      "outputs": [],
      "source": [
        "def jitter(seq, std=1):\n",
        "    n = len(seq)\n",
        "    return np.random.normal(0, std, n) + seq"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "696a9d53",
      "metadata": {
        "id": "696a9d53"
      },
      "source": [
        "If we jitter the scores with a standard deviation of 3, the rows and columns are no longer visible in the scatter plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30e7bb4a",
      "metadata": {
        "id": "30e7bb4a"
      },
      "outputs": [],
      "source": [
        "sat_verbal_jittered = jitter(sat_verbal, 3)\n",
        "sat_math_jittered = jitter(sat_math, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "feb909a3",
      "metadata": {
        "id": "feb909a3"
      },
      "outputs": [],
      "source": [
        "plt.scatter(sat_verbal_jittered, sat_math_jittered, s=5)\n",
        "\n",
        "decorate(xlabel=\"SAT Verbal\", ylabel=\"SAT Math\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7ef7d8f",
      "metadata": {
        "id": "d7ef7d8f"
      },
      "source": [
        "Jittering reduces the visual effect of rounding and makes the shape of the relationship clearer.\n",
        "But in general you should only jitter data for purposes of visualization and avoid using jittered data for analysis.\n",
        "\n",
        "In this example, even after adjusting the marker size and jittering the data, there is still some overplotting.\n",
        "So let's try one more thing: we can use the `alpha` keyword to make the markers partly transparent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "267e85f3",
      "metadata": {
        "id": "267e85f3"
      },
      "outputs": [],
      "source": [
        "plt.scatter(sat_verbal_jittered, sat_math_jittered, s=5, alpha=0.2)\n",
        "\n",
        "decorate(xlabel=\"SAT Verbal\", ylabel=\"SAT Math\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63cd6085",
      "metadata": {
        "id": "63cd6085"
      },
      "source": [
        "With transparency, overlapping data points look darker, so darkness is proportional to density.\n",
        "\n",
        "Although scatter plots are a simple and widely-used visualization, they can be hard to get right.\n",
        "In general, it takes some trial and error to adjust marker sizes, transparency, and jittering to find the best visual representation of the relationship between variables."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85e28ec5",
      "metadata": {
        "id": "85e28ec5"
      },
      "source": [
        "## Decile Plots\n",
        "\n",
        "Scatter plots provide a general impression of the relationship between variables, but there are other visualizations that provide more insight into the nature of the relationship.\n",
        "One of them is a **decile plot**.\n",
        "\n",
        "To generate a decile plot, we'll sort the respondents by verbal score and divide them into 10 groups, called **deciles**.\n",
        "We can use the `qcut` method to compute the deciles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c74cc154",
      "metadata": {
        "id": "c74cc154"
      },
      "outputs": [],
      "source": [
        "deciles = pd.qcut(nlsy_valid[\"sat_verbal\"], 10, labels=False) + 1\n",
        "deciles.value_counts().sort_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b217759e",
      "metadata": {
        "id": "b217759e"
      },
      "source": [
        "The number of respondents in each decile is roughly equal.\n",
        "\n",
        "Now we can use the `groupby` method to divide the `DataFrame` into groups by `decile`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc2d2294",
      "metadata": {
        "id": "fc2d2294"
      },
      "outputs": [],
      "source": [
        "df_groupby = nlsy_valid.groupby(deciles)\n",
        "df_groupby"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4a1bd59",
      "metadata": {
        "id": "e4a1bd59"
      },
      "source": [
        "The result is a `DataFrameGroupBy` object that represents the groups.\n",
        "We can select the `sat_math` column from it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "316b9219",
      "metadata": {
        "id": "316b9219"
      },
      "outputs": [],
      "source": [
        "series_groupby = df_groupby[\"sat_math\"]\n",
        "series_groupby"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d4af738",
      "metadata": {
        "id": "7d4af738"
      },
      "source": [
        "The result is a `SeriesGroupBy` object that represents the math scores in each decile.\n",
        "We can use the `quantile` function to compute the 10th, 50th, and 90th percentiles in each group."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f2dc655",
      "metadata": {
        "id": "0f2dc655"
      },
      "outputs": [],
      "source": [
        "low = series_groupby.quantile(0.1)\n",
        "median = series_groupby.quantile(0.5)\n",
        "high = series_groupby.quantile(0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ef92e8e",
      "metadata": {
        "id": "8ef92e8e"
      },
      "source": [
        "A decile plot shows these percentiles for each decile group.\n",
        "In the following figure, the line shows the median and the shaded region shows the area between the 10th and 90th percentiles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33bebae6",
      "metadata": {
        "id": "33bebae6"
      },
      "outputs": [],
      "source": [
        "xs = median.index\n",
        "plt.fill_between(xs, low, high, alpha=0.2)\n",
        "plt.plot(xs, median, label=\"median\")\n",
        "\n",
        "decorate(xlabel=\"SAT Verbal Decile\", ylabel=\"SAT Math\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba049565",
      "metadata": {
        "id": "ba049565"
      },
      "source": [
        "As an alternative, we can compute the median verbal score in each group and plot those values on the x-axis, rather than the decile numbers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7079db5e",
      "metadata": {
        "id": "7079db5e"
      },
      "outputs": [],
      "source": [
        "xs = df_groupby[\"sat_verbal\"].median()\n",
        "\n",
        "plt.fill_between(xs, low, high, alpha=0.2)\n",
        "plt.plot(xs, median, color=\"C0\", label=\"median\")\n",
        "\n",
        "decorate(xlabel=\"SAT Verbal\", ylabel=\"SAT Math\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27526e6c",
      "metadata": {
        "id": "27526e6c"
      },
      "source": [
        "It looks like the relationship between these variables is linear -- that is, each increase in the median verbal scores corresponds to a roughly equal increase in median math scores.\n",
        "\n",
        "More generally, we could divide the respondents into any number of groups, not necessarily 10, and we could compute other summary statistics in each group, not just these percentiles."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "503b7c6e",
      "metadata": {
        "id": "503b7c6e"
      },
      "source": [
        "## Correlation\n",
        "\n",
        "When the NLSY participants were in 9th grade, many of them took the mathematics section of the Peabody Individual Achievement Test (PIAT).\n",
        "Let's give the column that contains the results a more interpretable name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db6aaf59",
      "metadata": {
        "id": "db6aaf59"
      },
      "outputs": [],
      "source": [
        "nlsy[\"piat_math\"] = nlsy[\"R1318200\"]\n",
        "nlsy[\"piat_math\"].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b4e4716",
      "metadata": {
        "tags": [],
        "id": "4b4e4716"
      },
      "source": [
        "Here's what the distribution of scores looks like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ace951d",
      "metadata": {
        "tags": [],
        "id": "9ace951d"
      },
      "outputs": [],
      "source": [
        "from empiricaldist import Cdf\n",
        "\n",
        "cdf_piat_math = Cdf.from_seq(nlsy[\"piat_math\"], name=\"PIAT math\")\n",
        "cdf_piat_math.step()\n",
        "decorate(ylabel=\"CDF\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "472b6f2e",
      "metadata": {
        "id": "472b6f2e"
      },
      "source": [
        "Students who do well on the PIAT in 9th grade are likely to do well on the SAT math section in 12th grade.\n",
        "For the NLSY participants who took both tests, the following scatter plot shows the relationship between their scores.\n",
        "It uses the `scatter` function in `thinkstats`, which adjusts the marker size and transparency, and optionally jitters the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1ab698c",
      "metadata": {
        "id": "e1ab698c"
      },
      "outputs": [],
      "source": [
        "from thinkstats import scatter\n",
        "\n",
        "scatter(nlsy, \"piat_math\", \"sat_math\")\n",
        "\n",
        "decorate(xlabel=\"PIAT Math\", ylabel=\"SAT Math\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2157c0c7",
      "metadata": {
        "id": "2157c0c7"
      },
      "source": [
        "As expected, students who do well on the PIAT are likely to do well on the SAT math.\n",
        "And if math and verbal ability are related, we expect them do well on the SAT verbal section, too.\n",
        "The following figure shows the relationship between the PIAT and SAT verbal scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1f033da",
      "metadata": {
        "id": "f1f033da"
      },
      "outputs": [],
      "source": [
        "scatter(nlsy, \"piat_math\", \"sat_verbal\")\n",
        "\n",
        "decorate(xlabel=\"PIAT Math\", ylabel=\"SAT Verbal\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a4ce427",
      "metadata": {
        "id": "0a4ce427"
      },
      "source": [
        "Students with higher PIAT scores also have higher SAT verbal scores, on average.\n",
        "\n",
        "Comparing the scatter plots, the points in the first figure might be more compact, and the points in the second figure more dispersed.\n",
        "If so, that means that the PIAT math scores predict SAT math scores more accurately than they predict SAT verbal scores -- and it makes sense if they do.\n",
        "\n",
        "To quantify the strength of these relationships, we can use the **Pearson correlation coefficient**, often just called \"correlation\".\n",
        "To understand correlation, let's start with standardization.\n",
        "\n",
        "To standardize a variable, we subtract off the mean and divide through by the standard deviation, as in this function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19abd26d",
      "metadata": {
        "id": "19abd26d"
      },
      "outputs": [],
      "source": [
        "def standardize(xs):\n",
        "    \"\"\"Standardizes a sequence of numbers.\n",
        "\n",
        "    xs: sequence of numbers\n",
        "\n",
        "    returns: NumPy array\n",
        "    \"\"\"\n",
        "    return (xs - np.mean(xs)) / np.std(xs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2ffd0a5",
      "metadata": {
        "id": "a2ffd0a5"
      },
      "source": [
        "To show how it's used, we'll select the rows where `piat_math` and `sat_math` are valid."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e86cfdab",
      "metadata": {
        "id": "e86cfdab"
      },
      "outputs": [],
      "source": [
        "valid = nlsy.dropna(subset=[\"piat_math\", \"sat_math\"])\n",
        "piat_math = valid[\"piat_math\"]\n",
        "sat_math = valid[\"sat_math\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a610382c",
      "metadata": {
        "id": "a610382c"
      },
      "source": [
        "And standardize the PIAT math scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "446d2357",
      "metadata": {
        "id": "446d2357"
      },
      "outputs": [],
      "source": [
        "piat_math_standard = standardize(piat_math)\n",
        "np.mean(piat_math_standard), np.std(piat_math_standard)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f2e36a4",
      "metadata": {
        "id": "5f2e36a4"
      },
      "source": [
        "The results are **standard scores**, also called \"z-scores\".\n",
        "Because of the way the standard scores are calculated, the mean is close to 0 and the standard deviation is close to 1.\n",
        "\n",
        "Let's also standardize the SAT math scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf067f1a",
      "metadata": {
        "id": "cf067f1a"
      },
      "outputs": [],
      "source": [
        "sat_math_standard = standardize(sat_math)\n",
        "np.mean(sat_math_standard), np.std(sat_math_standard)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8486034c",
      "metadata": {
        "id": "8486034c"
      },
      "source": [
        "The following figure shows sequences of these scores for the first 100 participants."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3de994e",
      "metadata": {
        "tags": [],
        "id": "b3de994e"
      },
      "source": [
        "Calling `subplot` with the arguments `2, 1, 1` tells Matplotlib to create multiple plots, arranged in two rows and one column, and initializes the first plot.\n",
        "Calling it again with the arguments `2, 1, 2` initializes the second plot.\n",
        "`axhline` draws a horizontal line that spans the width of the axes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "391880a1",
      "metadata": {
        "tags": [],
        "id": "391880a1"
      },
      "outputs": [],
      "source": [
        "plt.subplot(2, 1, 1)\n",
        "plt.axhline(0, color=\"gray\", lw=1, alpha=0.5)\n",
        "plt.plot(piat_math_standard.values[:100], label=\"PIAT math\")\n",
        "decorate(ylabel=\"z-score\", xticks=[])\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.axhline(0, color=\"gray\", lw=1, alpha=0.5)\n",
        "plt.plot(sat_math_standard.values[:100], label=\"SAT math\", color=\"C1\")\n",
        "decorate(ylabel=\"z-score\", xticks=[])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07ae4459",
      "metadata": {
        "id": "07ae4459"
      },
      "source": [
        "These variables are clearly related: when one is above the mean, the other is likely to be above the mean, too.\n",
        "To quantify the strength of this relationship, we'll multiply the standard scores element-wise and compute the average of the products.\n",
        "\n",
        "When both scores are positive, their product is positive, so it tends to increase the average product.\n",
        "And when both scores are negative, their product is positive, so it also tends to increase the average product.\n",
        "When the scores have opposite signs, the product is negative, so it decreases the average product.\n",
        "As a result, the average product measures the similarity between the sequences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5373a730",
      "metadata": {
        "id": "5373a730"
      },
      "outputs": [],
      "source": [
        "np.mean(piat_math_standard * sat_math_standard)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b7351bf",
      "metadata": {
        "id": "4b7351bf"
      },
      "source": [
        "The result, which is about 0.64, is the correlation coefficient.\n",
        "Here's one way to interpret it: if someone's PIAT math score is 1 standard deviation above the mean, we expect their SAT math score to be 0.64 standard deviations above the mean, on average.\n",
        "\n",
        "The result is the same if we multiply the elements in the other order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc6e256d",
      "metadata": {
        "id": "dc6e256d"
      },
      "outputs": [],
      "source": [
        "np.mean(sat_math_standard * piat_math_standard)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4567ddce",
      "metadata": {
        "id": "4567ddce"
      },
      "source": [
        "So the correlation coefficient is symmetric: if someone's SAT math score is 1 standard deviation above the mean, we expect their PIAT math score to be 0.64 standard deviations above the mean, on average.\n",
        "\n",
        "Correlation is a commonly-used statistic, so NumPy provides a function that computes it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0227198",
      "metadata": {
        "id": "c0227198"
      },
      "outputs": [],
      "source": [
        "np.corrcoef(piat_math, sat_math)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f3919d9",
      "metadata": {
        "id": "3f3919d9"
      },
      "source": [
        "The result is a **correlation matrix**, with one row and one column for each variable.\n",
        "The value in the upper left is the correlation of `piat_math` with itself.\n",
        "The value in the lower right is the correlation of `sat_math` with itself.\n",
        "The correlation of any variable with itself is 1, which indicates perfect correlation.\n",
        "\n",
        "The values in the upper right and lower left are the correlation of `piat_math` with `sat_math` and the correlation of `sat_math` with `piat_math`, which are necessarily equal.\n",
        "\n",
        "`thinkstats` provides a `corrcoef` function that takes a `DataFrame` and two column names, selects the rows where both columns are valid, and computes their correlation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "846e2cb5",
      "metadata": {
        "id": "846e2cb5"
      },
      "outputs": [],
      "source": [
        "from thinkstats import corrcoef\n",
        "\n",
        "corrcoef(nlsy, \"piat_math\", \"sat_math\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b8d5a6c",
      "metadata": {
        "id": "9b8d5a6c"
      },
      "source": [
        "We can use this function to compute the correlation of `piat_math` and `sat_verbal`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37e14193",
      "metadata": {
        "id": "37e14193"
      },
      "outputs": [],
      "source": [
        "corrcoef(nlsy, \"piat_math\", \"sat_verbal\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29a98516",
      "metadata": {
        "id": "29a98516"
      },
      "source": [
        "The correlation is about 0.51, so if someone's PIAT math score is one standard deviation above the mean, we expect their SAT verbal score to be 0.51 standard deviations above the mean, on average.\n",
        "\n",
        "As we might expect, PIAT math scores predict SAT math scores better than they predict SAT verbal scores."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c19d94ec",
      "metadata": {
        "id": "c19d94ec"
      },
      "source": [
        "## Strength of Correlation\n",
        "\n",
        "As you look at more scatter plots, you will get a sense of what different correlations look like.\n",
        "To help you develop this sense, the following figure shows scatter plots for randomly-generated data with the different correlations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6eb14d8",
      "metadata": {
        "tags": [],
        "id": "a6eb14d8"
      },
      "outputs": [],
      "source": [
        "np.random.seed(17)\n",
        "xs = np.random.normal(size=300)\n",
        "ys = np.random.normal(size=300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b70be932",
      "metadata": {
        "tags": [],
        "id": "b70be932"
      },
      "outputs": [],
      "source": [
        "from thinkstats import make_correlated_scatter\n",
        "\n",
        "plt.figure(figsize=(10, 2.5))\n",
        "\n",
        "for i, rho in enumerate([0, 0.3, 0.7, 0.99]):\n",
        "    plt.subplot(1, 4, i + 1)\n",
        "    make_correlated_scatter(xs, ys, rho)\n",
        "decorate()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75ec51a8",
      "metadata": {
        "id": "75ec51a8"
      },
      "source": [
        "The Greek letter Ï, which is spelled \"rho\" and pronounced like \"row\", is the conventional symbol for the correlation coefficient.\n",
        "\n",
        "Correlation can also be negative.\n",
        "Here are scatter plots for random data with a range of negative correlations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbe1a470",
      "metadata": {
        "tags": [],
        "id": "fbe1a470"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 2.5))\n",
        "\n",
        "for i, rho in enumerate([-0.1, -0.3, -0.7, -0.99]):\n",
        "    plt.subplot(1, 4, i + 1)\n",
        "    make_correlated_scatter(xs, ys, rho)\n",
        "decorate()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eaf40c0f",
      "metadata": {
        "id": "eaf40c0f"
      },
      "source": [
        "The correlation coefficient is always between -1 and 1.\n",
        "If there is no relationship between two variables, their correlation is 0 -- but if the correlation is 0, that doesn't necessarily mean there is no relationship.\n",
        "\n",
        "In particular, if there is a non-linear relationship, the correlation coefficient can be close to 0.\n",
        "In each of the following examples, there is a clear relationship between the variables in the sense that if we are given one of the values, we can make a substantially better prediction of the other.\n",
        "But in each case the correlation coefficient is close to 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55bb0f69",
      "metadata": {
        "tags": [],
        "id": "55bb0f69"
      },
      "outputs": [],
      "source": [
        "from thinkstats import make_nonlinear_scatter\n",
        "\n",
        "plt.figure(figsize=(10, 2.5))\n",
        "\n",
        "for i, kind in enumerate([\"abs\", \"quadratic\", \"sinusoid\"]):\n",
        "    plt.subplot(1, 4, i + 1)\n",
        "    make_nonlinear_scatter(xs, ys, kind)\n",
        "decorate()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "813492ce",
      "metadata": {
        "id": "813492ce"
      },
      "source": [
        "Correlation quantifies the strength of a *linear* relationship between variables.\n",
        "If there is a non-linear relationship, the correlation coefficient can be misleading.\n",
        "And if the correlation is close to 0, that does *not* mean there is no relationship."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc7d3927",
      "metadata": {
        "id": "fc7d3927"
      },
      "source": [
        "## Rank Correlation\n",
        "\n",
        "\n",
        "The NLSY is longitudinal, which means that it follows the same group of people over time.\n",
        "The group we've been studying includes people born between 1980 and 1984.\n",
        "The ones who took the SAT probably took it in the late 1990s, when they were about 18 years old.\n",
        "So when they were asked about their income in 2021, they were in their late 30s or early 40s.\n",
        "Let's give the column with the income data a more interpretable name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e84e6376",
      "metadata": {
        "id": "e84e6376"
      },
      "outputs": [],
      "source": [
        "nlsy[\"income\"] = nlsy[\"U4949700\"]\n",
        "nlsy[\"income\"].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d8cbcbb",
      "metadata": {
        "id": "0d8cbcbb"
      },
      "source": [
        "The values in this column are gross family income, which is total income of the respondent and the other members of their household, from all sources, reported in U.S. dollars (USD).\n",
        "Here's what the distribution of income looks like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02490eb3",
      "metadata": {
        "id": "02490eb3"
      },
      "outputs": [],
      "source": [
        "cdf_income = Cdf.from_seq(nlsy[\"income\"])\n",
        "cdf_income.step()\n",
        "\n",
        "decorate(xlabel=\"Income (USD)\", ylabel=\"CDF\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd5fee5a",
      "metadata": {
        "id": "fd5fee5a"
      },
      "source": [
        "Notice the step near $600,000 -- values above this threshold were capped to protect the anonymity of the participants.\n",
        "Now here's a scatter plot of the respondents' SAT math scores and their income later in life."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "122dfbe2",
      "metadata": {
        "id": "122dfbe2"
      },
      "outputs": [],
      "source": [
        "scatter(nlsy, \"piat_math\", \"income\")\n",
        "\n",
        "decorate(xlabel=\"PIAT math\", ylabel=\"Gross Family Income (USD)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05f8fc85",
      "metadata": {
        "id": "05f8fc85"
      },
      "source": [
        "It looks like there is a relationship between these variables.\n",
        "Here is the correlation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "805329c5",
      "metadata": {
        "id": "805329c5"
      },
      "outputs": [],
      "source": [
        "corrcoef(nlsy, \"piat_math\", \"income\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed93365a",
      "metadata": {
        "id": "ed93365a"
      },
      "source": [
        "The correlation is about 0.3, which means that if someone gets a PIAT math score one standard deviation above the mean when they are 15 years old, we expect their income to be about 0.3 standard deviations above the mean when they are 40.\n",
        "That's not as strong as the correlation between PIAT scores and SAT scores, but considering the number of factors that affect income, it's pretty strong.\n",
        "\n",
        "In fact, Pearson's correlation coefficient might understate the strength of the relationship.\n",
        "As we can see in the previous scatter plot, both variables have an apparent excess of values at the extremes.\n",
        "Because the correlation coefficient is based on the product of deviations from the mean, it is sensitive to these extreme values.\n",
        "\n",
        "A more robust alternative is the **rank correlation**, which is based on the ranks of the scores rather than standardized scores.\n",
        "We can use the Pandas method `rank` to compute the rank of each score and each income."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb84c189",
      "metadata": {
        "id": "fb84c189"
      },
      "outputs": [],
      "source": [
        "valid = nlsy.dropna(subset=[\"piat_math\", \"income\"])\n",
        "\n",
        "piat_math_rank = valid[\"piat_math\"].rank(method=\"first\")\n",
        "income_rank = valid[\"income\"].rank(method=\"first\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8a6704d",
      "metadata": {
        "id": "d8a6704d"
      },
      "source": [
        "With the `method=\"first\"` argument, `rank` assigns ranks from 1 to the length of the sequence, which is 4101."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4baac236",
      "metadata": {
        "id": "4baac236"
      },
      "outputs": [],
      "source": [
        "income_rank.min(), income_rank.max()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a1aacca",
      "metadata": {
        "id": "2a1aacca"
      },
      "source": [
        "Here's a scatter plot of income ranks versus math score ranks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4470cb53",
      "metadata": {
        "id": "4470cb53"
      },
      "outputs": [],
      "source": [
        "plt.scatter(piat_math_rank, income_rank, s=5, alpha=0.2)\n",
        "\n",
        "decorate(xlabel=\"PIAT math rank\", ylabel=\"Income rank\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71fde67d",
      "metadata": {
        "id": "71fde67d"
      },
      "source": [
        "And here's the correlation of the ranks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9cfd2c2",
      "metadata": {
        "id": "f9cfd2c2"
      },
      "outputs": [],
      "source": [
        "np.corrcoef(piat_math_rank, income_rank)[0, 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17287efd",
      "metadata": {
        "id": "17287efd"
      },
      "source": [
        "The result is about 0.38, somewhat higher than the Pearson correlation, which is 0.30.\n",
        "Because rank correlation is less sensitive to the effect of extreme values, it is probably a better measure of the strength of the relationship between these variables.\n",
        "\n",
        "`thinkplot` provides a `rankcorr` function that encapsulates the code in this section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9def2d6d",
      "metadata": {
        "id": "9def2d6d"
      },
      "outputs": [],
      "source": [
        "from thinkstats import rankcorr\n",
        "\n",
        "rankcorr(nlsy, \"piat_math\", \"income\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "375c01a0",
      "metadata": {
        "tags": [],
        "id": "375c01a0"
      },
      "source": [
        "And SciPy provides a similar function called `spearmanr`, because rank correlation is also called Spearman's correlation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53c7ab46",
      "metadata": {
        "tags": [],
        "id": "53c7ab46"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import spearmanr\n",
        "\n",
        "spearmanr(valid[\"piat_math\"], valid[\"income\"]).statistic"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4d26d41",
      "metadata": {
        "id": "a4d26d41"
      },
      "source": [
        "As an exercise, you'll have a chance to compute the correlation between SAT verbal scores and income, using both Pearson correlation and rank correlation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22a5b4cd",
      "metadata": {
        "id": "22a5b4cd"
      },
      "source": [
        "## Correlation and Causation\n",
        "\n",
        "If variables A and B are correlated, the apparent correlation might be due to random sampling, or it might be the result of non-representative sampling, or it might indicate a real correlation between quantities in the population.\n",
        "\n",
        "If the correlation is real, there are three possible explanations: A causes B, or B causes A, or some other set of factors causes both A and B. These explanations are called \"causal relationships\".\n",
        "\n",
        "Correlation alone does not distinguish between these explanations, so it does not tell you which ones are true.\n",
        "This rule is often summarized with the phrase \"Correlation does not imply causation,\" which is so pithy it has its own Wikipedia page."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06983fb0",
      "metadata": {
        "tags": [],
        "id": "06983fb0"
      },
      "source": [
        "<http://wikipedia.org/wiki/Correlation_does_not_imply_causation>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f4ace1e",
      "metadata": {
        "id": "1f4ace1e"
      },
      "source": [
        "So what can you do to provide evidence of causation?\n",
        "\n",
        "1.  Use time. If A comes before B, then A can cause B but not the other way around.\n",
        "The order of events can help us infer the direction of causation, but it does not preclude the possibility that something else causes both A and B.\n",
        "\n",
        "2.  Use randomness.\n",
        "If you divide a large sample into two groups at random and compute the means of almost any variable, you expect the difference to be small.\n",
        "If the groups are nearly identical in all variables but A and B, you can rule out the possibility that something else causes both A and B.\n",
        "\n",
        "These ideas are the motivation for the **randomized controlled trial**, in which subjects are assigned randomly to two (or more) groups: a **treatment group** that receives some kind of intervention, like a new medicine, and a **control group** that receives no intervention, or another treatment whose effects are known.\n",
        "A randomized controlled trial is the most reliable way to demonstrate a causal relationship, and the foundation of evidence-based medicine.\n",
        "\n",
        "Unfortunately, controlled trials are sometimes impossible or unethical.\n",
        "An alternative is to look for a **natural experiment**, where similar groups are exposed to different conditions due to circumstances beyond the control of the experimenter.\n",
        "\n",
        "Identifying and measuring causal relationships is the topic of a branch of statistics called **causal inference**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4586192",
      "metadata": {
        "id": "c4586192"
      },
      "source": [
        "## Glossary\n",
        "\n",
        "-   **scatter plot**: A visualization that shows the relationship between two variables by plotting one point for each observation in the dataset.\n",
        "\n",
        "-   **overplotted:** A scatter plot is overplotted if many markers overlap, making it hard to distinguish areas of different density, which can misrepresent the relationship.\n",
        "\n",
        "-   **jitter**: Random noise added to data points in a plot to make overlapping values more visible.\n",
        "\n",
        "-   **decile plot:** A plot that divides data into deciles (ten groups) based on one variable, then summarizes another variable for each group.\n",
        "\n",
        "-   **decile:** One of the groups created by sorting data and dividing it into ten roughly equal parts.\n",
        "\n",
        "- **Pearson correlation coefficient:** A statistic that measures the strength and sign (positive or negative) of the linear relationship between two variables.\n",
        "\n",
        "-   **standard score**: A quantity that has been standardized so that it is expressed in standard deviations from the mean.\n",
        "\n",
        "-   **correlation matrix:** A table showing the correlation coefficients for each pair of variables in a dataset.\n",
        "\n",
        "-   **rank correlation**: A robust way to quantify the strength of a relationship by using the ranks of values instead of the actual values.\n",
        "\n",
        "-   **randomized controlled trial**: An experiment where subjects are randomly assigned to groups that receive different treatments.\n",
        "\n",
        "-   **treatment group**: In an experiment, the group that receives the intervention being tested.\n",
        "\n",
        "-   **control group**: In an experiment, the group that does not receive the intervention, or receives a treatment whose effect is known.\n",
        "\n",
        "-   **natural experiment**: An experiment that uses naturally occurring groups, which can sometimes mimic random assignment.\n",
        "\n",
        "-   **causal inference:** Methods for identifying and quantifying cause-and-effect relationships.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89eff46b",
      "metadata": {
        "id": "89eff46b"
      },
      "source": [
        "## Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6ca158a",
      "metadata": {
        "id": "b6ca158a"
      },
      "source": [
        "### Exercise 7.1\n",
        "\n",
        "The `thinkstats` module provides a function called `decile_plot` that encapsulates the code from earlier in this chapter.\n",
        "We can call it like this to visualize the relationship between SAT verbal and math scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6f6a419",
      "metadata": {
        "id": "d6f6a419"
      },
      "outputs": [],
      "source": [
        "from thinkstats import decile_plot\n",
        "\n",
        "decile_plot(nlsy, \"sat_verbal\", \"sat_math\")\n",
        "decorate(xlabel=\"SAT Verbal\", ylabel=\"SAT Math\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c48e4f9c",
      "metadata": {
        "id": "c48e4f9c"
      },
      "source": [
        "Make a decile plot of PIAT math scores and income.\n",
        "Does it appear to be a linear relationship?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "253c3774",
      "metadata": {
        "id": "253c3774"
      },
      "outputs": [],
      "source": [
        "# Solution goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a462b376",
      "metadata": {
        "id": "a462b376"
      },
      "source": [
        "### Exercise 7.2\n",
        "\n",
        "Make a scatter plot of income versus SAT math scores.\n",
        "Compute Pearson's correlation and rank correlation.\n",
        "Are they substantially different?\n",
        "\n",
        "Make a scatter plot of income versus SAT verbal scores, and compute both correlations.\n",
        "Which is a stronger prediction of future income, math or verbal scores?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c451726",
      "metadata": {
        "id": "0c451726"
      },
      "outputs": [],
      "source": [
        "# Solution goes here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c09e741",
      "metadata": {
        "id": "4c09e741"
      },
      "outputs": [],
      "source": [
        "# Solution goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8988e57e",
      "metadata": {
        "id": "8988e57e"
      },
      "source": [
        "### Exercise 7.3\n",
        "\n",
        "Let's see how a student's high school grade point average (GPA) is correlated with their SAT scores.\n",
        "Here's the variable in the NLSY dataset that encodes GPA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b4bae3d",
      "metadata": {
        "id": "3b4bae3d"
      },
      "outputs": [],
      "source": [
        "missing_codes = [-6, -7, -8, -9]\n",
        "nlsy[\"gpa\"] = nlsy[\"R9871900\"].replace(missing_codes, np.nan) / 100\n",
        "nlsy[\"gpa\"].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22df5e3a",
      "metadata": {
        "tags": [],
        "id": "22df5e3a"
      },
      "source": [
        "And here's what the distribution of GPAs looks like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9c42969",
      "metadata": {
        "tags": [],
        "id": "b9c42969"
      },
      "outputs": [],
      "source": [
        "cdf_income = Cdf.from_seq(nlsy[\"gpa\"])\n",
        "cdf_income.step()\n",
        "decorate(xlabel=\"GPA\", ylabel=\"CDF\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e3409ce",
      "metadata": {
        "id": "6e3409ce"
      },
      "source": [
        "Make a scatter plot that shows the relationship between GPA and SAT math scores and compute the correlation coefficient.\n",
        "Do the same for the relationship between GPA and SAT verbal scores.\n",
        "Which SAT score is a better predictor of GPA?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ac28b4a",
      "metadata": {
        "id": "8ac28b4a"
      },
      "outputs": [],
      "source": [
        "# Solution goes here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79d6b9d0",
      "metadata": {
        "id": "79d6b9d0"
      },
      "outputs": [],
      "source": [
        "# Solution goes here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa23e11a",
      "metadata": {
        "id": "fa23e11a"
      },
      "outputs": [],
      "source": [
        "# Solution goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88d7b0f8",
      "metadata": {
        "id": "88d7b0f8"
      },
      "source": [
        "### Exercise 7.4\n",
        "\n",
        "Let's investigate the relationship between education and income.\n",
        "The NLSY dataset includes a column that reports the highest degree earned by each respondent.\n",
        "The values are encoded as integers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1fa8729",
      "metadata": {
        "id": "f1fa8729"
      },
      "outputs": [],
      "source": [
        "nlsy[\"degree\"] = nlsy[\"Z9083900\"]\n",
        "nlsy[\"degree\"].value_counts().sort_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed6dfbbb",
      "metadata": {
        "id": "ed6dfbbb"
      },
      "source": [
        "But we can use these lists to decode them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfdc04d4",
      "metadata": {
        "id": "cfdc04d4"
      },
      "outputs": [],
      "source": [
        "positions = [0, 1, 2, 3, 4, 5, 6, 7]\n",
        "labels = [\n",
        "    \"None\",\n",
        "    \"GED\",\n",
        "    \"High school diploma\",\n",
        "    \"Associate's degree\",\n",
        "    \"Bachelor's degree\",\n",
        "    \"Master's degree\",\n",
        "    \"PhD\",\n",
        "    \"Professional degree\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0edf50ee",
      "metadata": {
        "tags": [],
        "id": "0edf50ee"
      },
      "source": [
        "And make a `Pmf` that represents the distribution of educational attainment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f496ca9c",
      "metadata": {
        "tags": [],
        "id": "f496ca9c"
      },
      "outputs": [],
      "source": [
        "from empiricaldist import Pmf\n",
        "\n",
        "Pmf.from_seq(nlsy[\"degree\"]).bar()\n",
        "\n",
        "plt.xticks(positions, labels, rotation=30, ha=\"right\")\n",
        "decorate(ylabel=\"PMF\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95686a46",
      "metadata": {
        "id": "95686a46"
      },
      "source": [
        "Make a scatter plot of `income` versus `degree`.\n",
        "To avoid overplotting, jitter the values of `degree` and adjust the marker size and transparency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72d38760",
      "metadata": {
        "id": "72d38760"
      },
      "outputs": [],
      "source": [
        "# Solution goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b834847",
      "metadata": {
        "id": "2b834847"
      },
      "source": [
        "Use the `groupby` method to group respondents by `degree`.\n",
        "From the `DataFrameGroupBy` object, select the `income` column; then use the `quantile` method to compute the median, 10th and 90th percentiles in each group.\n",
        "Use `fill_between` to plot the region between the 10th and 90th percentiles, and use `plot` to plot the medians.\n",
        "\n",
        "What can you say about the income premium associated with each additional degree?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06165707",
      "metadata": {
        "id": "06165707"
      },
      "outputs": [],
      "source": [
        "# Solution goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ad16602",
      "metadata": {
        "id": "3ad16602"
      },
      "source": [
        "### Exercise 7.4\n",
        "\n",
        "The Behavioral Risk Factor Surveillance System (BRFSS) dataset includes self-reported heights and weights for about 400,000 respondents.\n",
        "Instructions for downloading the data are in the notebook for this chapter.\n",
        "\n",
        "Make a scatter plot that shows the relationship between height and weight.\n",
        "You might have to jitter the data to blur the visible rows and columns due to rounding.\n",
        "And with such a large sample, you will have to adjust the marker size and transparency to avoid overplotting.\n",
        "Also, because there are outliers in both measurements, you might want to use `xlim` and `ylim` to zoom in on a region that covers most of the respondents."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c2bf0cf",
      "metadata": {
        "tags": [],
        "id": "2c2bf0cf"
      },
      "source": [
        "Here's how we can load the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d079e1cc",
      "metadata": {
        "tags": [],
        "id": "d079e1cc"
      },
      "outputs": [],
      "source": [
        "download(\"https://github.com/AllenDowney/ThinkStats/raw/v3/data/CDBRFS08.ASC.gz\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0c95fd2",
      "metadata": {
        "tags": [],
        "id": "f0c95fd2"
      },
      "outputs": [],
      "source": [
        "from thinkstats import read_brfss\n",
        "\n",
        "brfss = read_brfss()\n",
        "brfss[\"htm3\"].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "113a01af",
      "metadata": {
        "id": "113a01af"
      },
      "outputs": [],
      "source": [
        "# Solution goes here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8918c1ea",
      "metadata": {
        "id": "8918c1ea"
      },
      "outputs": [],
      "source": [
        "# Solution goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9b0e975",
      "metadata": {
        "id": "c9b0e975"
      },
      "source": [
        "Make a decile plot of weight versus height. Does the relationship seem to be linear?\n",
        "Compute the correlation coefficient and rank correlation. Are they substantially different? Which one do you think better quantifies the relationship between these variables?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8573793",
      "metadata": {
        "id": "e8573793"
      },
      "outputs": [],
      "source": [
        "# Solution goes here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66c5d783",
      "metadata": {
        "id": "66c5d783"
      },
      "outputs": [],
      "source": [
        "# Solution goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e27da3a4",
      "metadata": {
        "tags": [],
        "id": "e27da3a4"
      },
      "source": [
        "[Think Stats: Exploratory Data Analysis in Python, 3rd Edition](https://allendowney.github.io/ThinkStats/index.html)\n",
        "\n",
        "Copyright 2024 [Allen B. Downey](https://allendowney.com)\n",
        "\n",
        "Code license: [MIT License](https://mit-license.org/)\n",
        "\n",
        "Text license: [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International](https://creativecommons.org/licenses/by-nc-sa/4.0/)"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Tags",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}